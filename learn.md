重点看：
- DerainDatasets.py
- networks.py
- train_PReNet.py
- test_PReNet.py

## 论文

### 模型

本文提出的方法是一种递归方法，图片通过 t 轮重复的结构完成推理。

设每次递归得到的结果是 $x^t$，而原始的带雨迹的图像为 $y$。

这个结构如下：
- $x^{t-1}$（上一轮的输出） 和 $y$ 拼接（2 张 RGB 图像，6 个通道）
- $f_{in}$，是一个单层卷积+ReLU，输入为6通道，输出为32通道
- （仅PReNet 有）一个 LSTM 层，将上一次递归的 LSTM 层的输出和 $f_{in}$ 的输出结合起来。
- $f_{res}$，是 5 个残差块堆叠成的，这5个残差块可以各自带有独立的参数，从而形成不带 r 的版本 PRN 和 PReNet；也可以是递归的（通过一个块，重复5遍），从而形成带 r 的版本 PRN$_r$ 和 PReNet$_r$。实验证实，带 r 的变种能够以效果稍微变差的代价来换取参数量的显著减少。
- $f_{out}$，是一个单层卷积

于是形成 4 种模型：
- 不带 LSTM 结构的 PRN 和它的 r 变种 PRN$_r$
- 带 LSTM 结构的 PReNet 和它的 r 变种 PReNet$_r$

在实验时还做出了一种输入不带 $y$（仅有 $x^{t-1}$）的版本 PReNet$_x$，这种模型架构先前已有人研究过。实验表明这种架构的表现明显不如带了 $y$ 的。

### 损失函数

在 MSE loss 和负 SSIM loss 上进行了实验，证明 SSIM 效果比 MSE 要好。

对于上节所述的循环处理，可以：
- 仅在最后输出时计算 SSIM loss（PReNet-SSIM），也可以
- 针对每轮输出都计算 SSIM loss 最后按权重加起来（PReNet-RecSSIM）
  
实验证明，二者最终可以达到几乎相同的训练效果，但 RecSSIM 训练出的模型在循环的早期阶段就有较好的效果。

图像越相似，SSIM 越大，负 SSIM 越小。因此用负 SSIM 作为 loss，寻求它的最小化，实际上是在寻求 SSIM 的最大化。

### 循环的轮数 T

实验表明，T 越大，最终效果越好。但更大的 T 会增加训练的成本，甚至到 T=7 时，模型的表现略有下降。因此最终选定 T=6。

## PyTorch

### 训练模式和评估模式

在PyTorch中，模型有两种模式：训练模式（training mode）和评估模式（evaluation mode）。这两种模式主要影响模型中的某些特定层，如Dropout和BatchNorm。

在训练模式下，Dropout层会随机地关闭一部分神经元以防止过拟合，BatchNorm层会使用mini-batch的统计信息进行数据的归一化。这两种操作都引入了一些随机性，有助于模型的训练。

然而，在评估模式下，我们希望模型的输出是确定的，不应受到任何随机性的影响。因此，我们需要关闭Dropout层的随机关闭操作，同时让BatchNorm层使用全局（从训练集中学习到的）统计信息进行归一化。这就是为什么我们需要将模型设置为评估模式的原因。

- 切换到训练模式：

    ```python
    model.train()
    ```

- 切换到评估模式：

    ```python
    model.eval()
    ```

一般训练时用训练模式，测试时用评估模式。

### Variable

用于表示 PyTorch 中的张量。现已弃用，应当以 `torch.Tensor` 代替之。

